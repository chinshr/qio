/*
**
** PHiPAC Matrix-Matrix Code for the operation:
**    C = alpha*transpose(A)*B + C
**
** Automatically Generated by mm_gen ($Revision: 1.1 $) using the command:
**    ../mm_gen/mm_gen -cb 1 2 10 -cb 100 12 5 -alpha c -opA T -file mul_mftmf_mf_alpha.c -rout mul_mftmf_mf_alpha 
**
** Run '../mm_gen/mm_gen -help' for help.
**
** Generated on: Tuesday December 12 1995, 18:11:56 PST
** Created by: Jeff Bilmes <bilmes@cs.berkeley.edu>
**             http://www.icsi.berkeley.edu/~bilmes/phipac
**
**
** Usage:
**    mul_mftmf_mf_alpha(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
** where
**  transpose(A) is an MxK matrix
**  B is an KxN matrix
**  C is an MxN matrix
**  Astride is the number of entries between the start of each row of transpose(A)
**  Bstride is the number of entries between the start of each row of B
**  Cstride is the number of entries between the start of each row of C
**
**
** "Copyright (c) 1995 The Regents of the University of California.  All
** rights reserved."  Permission to use, copy, modify, and distribute
** this software and its documentation for any purpose, without fee, and
** without written agreement is hereby granted, provided that the above
** copyright notice and the following two paragraphs appear in all copies
** of this software.
**
** IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
** DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT
** OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF
** CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
**
** THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES,
** INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
** AND FITNESS FOR A PARTICULAR PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS
** ON AN "AS IS" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO
** PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.
**
*/

#define ZERO1x1(c00) \
{\
   c00 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x1(c00,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; \
}

/* Fixed M,K,N = 1,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x1mf1x1_mf1x1(c00,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a0; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,1 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x2mf2x1_mf1x1(c00,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a0; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
}


#define ZERO1x2(c00,c01) \
{\
   c00 = 0.0; c01 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x2(c00,c01,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
}

/* Fixed M,K,N = 1,1,2 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x1mf1x2_mf1x2(c00,c01,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,2 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x2mf2x2_mf1x2(c00,c01,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; \
   A += Astride; \
}


#define ZERO1x4(c00,c01,c02,c03) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x4(c00,c01,c02,c03,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; \
}

/* Fixed M,K,N = 1,1,4 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x1mf1x4_mf1x4(c00,c01,c02,c03,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,4 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x2mf2x4_mf1x4(c00,c01,c02,c03,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; \
   A += Astride; \
}


#define ZERO1x8(c00,c01,c02,c03,c04,c05,c06,c07) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; c04 = 0.0; c05 = 0.0; c06 = 0.0; c07 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x8(c00,c01,c02,c03,c04,c05,c06,c07,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; _cp[4] += alpha*c04; _cp[5] += alpha*c05; _cp[6] += alpha*c06; _cp[7] += alpha*c07; \
}

/* Fixed M,K,N = 1,1,8 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x1mf1x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; c04 += _a0*_b4; c05 += _a0*_b5; c06 += _a0*_b6; c07 += _a0*_b7; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,8 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x2mf2x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; c04 += _a0*_b4; c05 += _a0*_b5; c06 += _a0*_b6; c07 += _a0*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; c04 += _a0*_b4; c05 += _a0*_b5; c06 += _a0*_b6; c07 += _a0*_b7; \
   A += Astride; \
}


#define ZERO1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; c04 = 0.0; c05 = 0.0; c06 = 0.0; c07 = 0.0; c08 = 0.0; c09 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; _cp[4] += alpha*c04; _cp[5] += alpha*c05; _cp[6] += alpha*c06; _cp[7] += alpha*c07; _cp[8] += alpha*c08; _cp[9] += alpha*c09; \
}

/* Fixed M,K,N = 1,1,10 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x1mf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; c04 += _a0*_b4; c05 += _a0*_b5; c06 += _a0*_b6; c07 += _a0*_b7; c08 += _a0*_b8; c09 += _a0*_b9; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,10 fully-unrolled matrix matrix multiply. */
#define mul_tmf1x2mf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a0; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; c04 += _a0*_b4; c05 += _a0*_b5; c06 += _a0*_b6; c07 += _a0*_b7; c08 += _a0*_b8; c09 += _a0*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; c01 += _a0*_b1; c02 += _a0*_b2; c03 += _a0*_b3; c04 += _a0*_b4; c05 += _a0*_b5; c06 += _a0*_b6; c07 += _a0*_b7; c08 += _a0*_b8; c09 += _a0*_b9; \
   A += Astride; \
}


/* Fixed M,N = 100,50, Arbitrary K L0-blocked matrix matrix multiply. */
static void
mul_mftmf_mf_alpha_l1_arb_k(int K, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0;
   const float *bp0;
   float *cp0;
   const int C_sbs_stride = Cstride*1;
   const int k_marg_el = K & 1;
   const int k_norm = (K - k_marg_el)*Astride;
   float *const c0_endp = C+100*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=1) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + 50;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,cp0,Cstride);
      }
   }
}

/* Arbitrary M,K,N L0-blocked matrix matrix multiply. */
static void
mul_mftmf_mf_alpha_l1_arb_all(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0;
   const float *bp0;
   float *cp0;
   const int C_sbs_stride = Cstride*1;
   const int k_marg_el = K & 1;
   const int k_norm = (K - k_marg_el)*Astride;
   const int m_marg_el = M & 0;
   const int m_norm = M - m_marg_el;
   const int n_marg_el = N % 10;
   const int n_norm = N - n_marg_el;
   float *const c0_endp = C+m_norm*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=1) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,cp0,Cstride);
      }
   }
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=1) {
      const float* const ap0_endp = a0 + k_norm;
      b0 = B+n_norm;
      cp0 = c0+n_norm;
      if (n_marg_el & 0x8) {
         ap0=a0;
         bp0=b0;
         ZERO1x8(c00,c01,c02,c03,c04,c05,c06,c07);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x8(c00,c01,c02,c03,c04,c05,c06,c07,cp0,Cstride);
         b0 += 8;
         cp0 += 8;
      }
      if (n_marg_el & 0x4) {
         ap0=a0;
         bp0=b0;
         ZERO1x4(c00,c01,c02,c03);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x4_mf1x4(c00,c01,c02,c03,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x4_mf1x4(c00,c01,c02,c03,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x4(c00,c01,c02,c03,cp0,Cstride);
         b0 += 4;
         cp0 += 4;
      }
      if (n_marg_el & 0x2) {
         ap0=a0;
         bp0=b0;
         ZERO1x2(c00,c01);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x2_mf1x2(c00,c01,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x2_mf1x2(c00,c01,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x2(c00,c01,cp0,Cstride);
         b0 += 2;
         cp0 += 2;
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0=b0;
         ZERO1x1(c00);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x1_mf1x1(c00,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x1_mf1x1(c00,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x1(c00,cp0,Cstride);
      }
   }
}

/* Fixed M,K,N = 100,24,50 L0-blocked matrix matrix multiply. */
static void
mul_mftmf_mf_alpha_l1(const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0;
   const float *bp0;
   float *cp0;
   const int C_sbs_stride = Cstride*1;
   const int k_norm = 24*Astride;
   float *const c0_endp = C+100*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=1) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + 50;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x2mf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,cp0,Cstride);
      }
   }
}

void
mul_mftmf_mf_alpha(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   /* Code for L1-blocked routine. */
   int m2,k2,n2;
   const float *a2,*b2;
   float *c2;
   const float *ap2,*bp2;
   float *cp2;
   if (alpha == 0.0) {
      return;
   }
   if (M < 101 && K < 25 && N < 51) {
      mul_mftmf_mf_alpha_l1_arb_all(M,K,N,A,B,C,Astride,Bstride,Cstride,alpha);
      return;
   }
   for (m2=0; m2<=M-100; m2+=100) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-50; n2+=50,b2+=50,cp2+=50) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-24; k2+=24,bp2+=24*Bstride,ap2+=24*Astride) {
            mul_mftmf_mf_alpha_l1(ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mul_mftmf_mf_alpha_l1_arb_k(K-k2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-24; k2+=24,bp2+=24*Bstride,ap2+=24*Astride) {
            mul_mftmf_mf_alpha_l1_arb_all(100,24,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mul_mftmf_mf_alpha_l1_arb_all(100,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
   if (m2 < M) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-50; n2+=50,b2+=50,cp2+=50) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-24; k2+=24,bp2+=24*Bstride,ap2+=24*Astride) {
            mul_mftmf_mf_alpha_l1_arb_all(M-m2,24,50,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mul_mftmf_mf_alpha_l1_arb_all(M-m2,K-k2,50,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-24; k2+=24,bp2+=24*Bstride,ap2+=24*Astride) {
            mul_mftmf_mf_alpha_l1_arb_all(M-m2,24,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mul_mftmf_mf_alpha_l1_arb_all(M-m2,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}
